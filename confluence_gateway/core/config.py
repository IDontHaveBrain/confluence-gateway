import json
import os
import platform
from pathlib import Path
from typing import Any, Literal, Optional, get_args

from pydantic import BaseModel, Field, HttpUrl, ValidationError, model_validator


class ConfluenceConfig(BaseModel):
    url: HttpUrl
    username: str
    api_token: str
    timeout: int = 10


class SearchConfig(BaseModel):
    default_limit: int = 20
    max_limit: int = 100
    default_expand: list[str] = ["body.view", "space"]


class EmbeddingConfig(BaseModel):
    provider: Literal["sentence-transformers", "litellm", "none"] = Field(
        default="none",
        description="The type of embedding provider to use ('sentence-transformers', 'litellm', or 'none').",
    )
    model_name: Optional[str] = Field(
        default=None,
        description="Name or path of the embedding model. "
        "Examples: 'all-MiniLM-L6-v2' (for sentence-transformers), "
        "'openai/text-embedding-ada-002', 'ollama/nomic-embed-text' (for litellm).",
    )
    dimension: Optional[int] = Field(
        default=None,
        description="Expected dimension of the embeddings generated by the model. "
        "Required if provider is not 'none'.",
    )
    litellm_api_key: Optional[str] = Field(
        default=None,
        description="API key for the LiteLLM provider (e.g., OpenAI, Cohere API key). "
        "Store securely, preferably via environment variable.",
        exclude=True,  # Prevent logging or accidental exposure
    )
    litellm_api_base: Optional[HttpUrl] = Field(
        default=None,
        description="API base URL for the LiteLLM provider. "
        "Required for providers like Azure OpenAI or self-hosted models (e.g., Ollama).",
    )
    device: Optional[Literal["cpu", "cuda"]] = Field(
        default=None,
        description="Device to use for sentence-transformers ('cpu' or 'cuda'). "
        "If None, it will auto-detect. Ignored for 'litellm' provider.",
    )

    @model_validator(mode="after")
    def check_conditional_requirements(self) -> "EmbeddingConfig":
        if self.provider != "none":
            if not self.model_name:
                raise ValueError(
                    "EMBEDDING_MODEL_NAME must be set if EMBEDDING_PROVIDER is not 'none'."
                )
            if self.dimension is None:
                raise ValueError(
                    "EMBEDDING_DIMENSION must be set if EMBEDDING_PROVIDER is not 'none'."
                )

        if self.provider == "litellm":
            if self.model_name and self.model_name.startswith("ollama/"):
                if not self.litellm_api_base:
                    raise ValueError(
                        "LITELLM_API_BASE must be set when using an 'ollama/' model with the 'litellm' provider."
                    )

        if self.provider == "sentence-transformers":
            if self.litellm_api_key or self.litellm_api_base:
                print(
                    "Warning: LITELLM_API_KEY and LITELLM_API_BASE are ignored when EMBEDDING_PROVIDER is 'sentence-transformers'."
                )
            elif self.provider == "litellm":
                if self.device:
                    print(
                        "Warning: EMBEDDING_DEVICE is ignored when EMBEDDING_PROVIDER is 'litellm'."
                    )

        return self


VectorDBType = Literal["chroma", "qdrant", "none"]


def get_user_config_path() -> Path:
    return Path.home() / ".confluence_gateway_config.json"


def _load_config_from_file(path: Path) -> dict[str, Any]:
    config_data = {}
    if path.exists() and path.is_file():
        try:
            with open(path, encoding="utf-8") as f:
                config_data = json.load(f)
            if not isinstance(config_data, dict):
                print(
                    f"Warning: Config file at {path} does not contain a valid JSON object. Ignoring."
                )
                return {}
            print(f"Info: Loaded configuration from {path}")
        except json.JSONDecodeError:
            print(
                f"Warning: Could not parse JSON from config file at {path}. Ignoring."
            )
            return {}
        except Exception as e:
            print(f"Warning: Error reading config file at {path}: {e}. Ignoring.")
            return {}
    return config_data


class VectorDBConfig(BaseModel):
    type: VectorDBType = Field(
        default="none", description="The type of vector database to use."
    )
    collection_name: str = Field(
        default="confluence_embeddings",
        description="Name of the collection/table in the vector database.",
    )
    embedding_dimension: Optional[int] = Field(
        default=None,
        description="Dimension of the text embeddings. Required if type is not 'none'.",
    )
    chunk_size: int = Field(
        default=512,
        description="Target size for text chunks during indexing.",
    )
    chunk_overlap: int = Field(
        default=50,
        description="Overlap between consecutive text chunks during indexing.",
    )

    chroma_persist_path: Optional[str] = Field(
        default=None,
        description="Path for ChromaDB persistent storage. Client mode takes precedence if host/port are set.",
    )
    chroma_host: Optional[str] = Field(
        default=None, description="Hostname for ChromaDB client/server mode."
    )
    chroma_port: Optional[int] = Field(
        default=None, description="Port for ChromaDB client/server mode."
    )

    qdrant_url: Optional[HttpUrl] = Field(
        default=None,
        description="URL for the Qdrant instance. Required if type is 'qdrant'.",
    )
    qdrant_api_key: Optional[str] = Field(
        default=None, description="API key for Qdrant authentication."
    )
    qdrant_grpc_port: int = Field(
        default=6334, description="Port for Qdrant gRPC connections."
    )
    qdrant_prefer_grpc: bool = Field(
        default=False, description="Prefer gRPC over HTTP for Qdrant."
    )

    @model_validator(mode="after")
    def check_conditional_requirements(self) -> "VectorDBConfig":
        if self.type != "none":
            if self.embedding_dimension is None:
                raise ValueError(
                    "VECTOR_DB_EMBEDDING_DIMENSION must be set if VECTOR_DB_TYPE is not 'none'."
                )

        if self.type == "qdrant":
            if self.qdrant_url is None:
                raise ValueError(
                    "QDRANT_URL must be set if VECTOR_DB_TYPE is 'qdrant'."
                )

        return self


def _load_raw_env_vars(prefix: str, case_sensitive: bool = False) -> dict[str, Any]:
    env_vars = {}

    is_windows = platform.system().lower() == "windows"
    effective_case_sensitive = case_sensitive and not is_windows

    for key, value in os.environ.items():
        if effective_case_sensitive:
            if key.startswith(prefix):
                config_key = key[len(prefix) :]
                env_vars[config_key] = value
        else:
            if key.upper().startswith(prefix.upper()):
                config_key = key[len(prefix) :].lower()
                env_vars[config_key] = value

    return env_vars


def _load_raw_search_env() -> dict[str, Any]:
    search_env = _load_raw_env_vars("SEARCH_")

    if "default_limit" in search_env and isinstance(search_env["default_limit"], str):
        try:
            search_env["default_limit"] = int(search_env["default_limit"])
        except ValueError:
            print("Warning: Invalid SEARCH_DEFAULT_LIMIT value, using default.")
            del search_env["default_limit"]
    if "max_limit" in search_env and isinstance(search_env["max_limit"], str):
        try:
            search_env["max_limit"] = int(search_env["max_limit"])
        except ValueError:
            print("Warning: Invalid SEARCH_MAX_LIMIT value, using default.")
            del search_env["max_limit"]
    if "default_expand" in search_env and isinstance(search_env["default_expand"], str):
        search_env["default_expand"] = [
            s.strip() for s in search_env["default_expand"].split(",") if s.strip()
        ]

    return search_env


def _load_raw_confluence_env() -> dict[str, Any]:
    confluence_env = _load_raw_env_vars("CONFLUENCE_")

    if "timeout" in confluence_env and isinstance(confluence_env["timeout"], str):
        try:
            confluence_env["timeout"] = int(confluence_env["timeout"])
        except ValueError:
            print("Warning: Invalid CONFLUENCE_TIMEOUT value, using default.")
            del confluence_env["timeout"]

    return confluence_env


def _load_raw_vector_db_env() -> dict[str, Any]:
    raw_config: dict[str, Any] = {}

    vector_db_type_str = os.getenv("VECTOR_DB_TYPE", "").lower()
    if vector_db_type_str:
        if vector_db_type_str not in get_args(VectorDBType):
            print(
                f"Warning: Invalid VECTOR_DB_TYPE '{vector_db_type_str}' in environment. Check config file or defaults."
            )
        else:
            raw_config["type"] = vector_db_type_str

    if dim_str := os.getenv("VECTOR_DB_EMBEDDING_DIMENSION"):
        try:
            raw_config["embedding_dimension"] = int(dim_str)
        except ValueError:
            raw_config["embedding_dimension"] = dim_str

    if col_name := os.getenv("VECTOR_DB_COLLECTION_NAME"):
        raw_config["collection_name"] = col_name

    if path := os.getenv("CHROMA_PERSIST_PATH"):
        raw_config["chroma_persist_path"] = path
    if host := os.getenv("CHROMA_HOST"):
        raw_config["chroma_host"] = host
    if port_str := os.getenv("CHROMA_PORT"):
        try:
            raw_config["chroma_port"] = int(port_str)
        except ValueError:
            raw_config["chroma_port"] = port_str

    if chunk_size_str := os.getenv("VECTOR_DB_CHUNK_SIZE"):
        try:
            raw_config["chunk_size"] = int(chunk_size_str)
        except ValueError:
            raw_config["chunk_size"] = chunk_size_str

    if chunk_overlap_str := os.getenv("VECTOR_DB_CHUNK_OVERLAP"):
        try:
            raw_config["chunk_overlap"] = int(chunk_overlap_str)
        except ValueError:
            raw_config["chunk_overlap"] = chunk_overlap_str

    if url := os.getenv("QDRANT_URL"):
        raw_config["qdrant_url"] = url
    if key := os.getenv("QDRANT_API_KEY"):
        raw_config["qdrant_api_key"] = key
    if grpc_port_str := os.getenv("QDRANT_GRPC_PORT"):
        try:
            raw_config["qdrant_grpc_port"] = int(grpc_port_str)
        except ValueError:
            raw_config["qdrant_grpc_port"] = grpc_port_str

    if prefer_grpc_str := os.getenv("QDRANT_PREFER_GRPC"):
        raw_config["qdrant_prefer_grpc"] = prefer_grpc_str.lower() in [
            "true",
            "1",
            "t",
            "yes",
            "y",
        ]

    return raw_config


def _load_raw_embedding_env() -> dict[str, Any]:
    raw_config: dict[str, Any] = {}

    provider_str = os.getenv("EMBEDDING_PROVIDER", "").lower()
    if provider_str:
        valid_providers = get_args(Literal["sentence-transformers", "litellm", "none"])
        if provider_str not in valid_providers:
            print(
                f"Warning: Invalid EMBEDDING_PROVIDER '{provider_str}' in environment. Check config file or defaults."
            )
        else:
            raw_config["provider"] = provider_str

    if model_name := os.getenv("EMBEDDING_MODEL_NAME"):
        raw_config["model_name"] = model_name

    if dim_str := os.getenv("EMBEDDING_DIMENSION"):
        try:
            raw_config["dimension"] = int(dim_str)
        except ValueError:
            raw_config["dimension"] = dim_str

    if api_key := os.getenv("LITELLM_API_KEY"):
        raw_config["litellm_api_key"] = api_key
    if api_base := os.getenv("LITELLM_API_BASE"):
        raw_config["litellm_api_base"] = api_base

    device_str = os.getenv("EMBEDDING_DEVICE", "").lower()
    if device_str:
        valid_devices = get_args(Optional[Literal["cpu", "cuda"]])
        valid_devices_str = [d for d in valid_devices if isinstance(d, str)]
        if device_str not in valid_devices_str:
            print(
                f"Warning: Invalid EMBEDDING_DEVICE '{device_str}' in environment. Using auto-detect or default."
            )
        else:
            raw_config["device"] = device_str

    return raw_config


def load_configurations() -> tuple[
    Optional[ConfluenceConfig],
    SearchConfig,
    Optional[VectorDBConfig],
    Optional[EmbeddingConfig],
]:
    user_config_path = get_user_config_path()
    file_config = _load_config_from_file(user_config_path)

    env_confluence_raw = _load_raw_confluence_env()
    env_search_raw = _load_raw_search_env()
    env_vector_db_raw = _load_raw_vector_db_env()
    env_embedding_raw = _load_raw_embedding_env()

    file_confluence = file_config.get("confluence", {})
    file_search = file_config.get("search", {})
    file_vector_db = file_config.get("vector_db", {})
    file_embedding = file_config.get("embedding", {})

    final_confluence_config = env_confluence_raw.copy()
    final_confluence_config.update(file_confluence)

    final_search_config = env_search_raw.copy()
    final_search_config.update(file_search)

    final_vector_db_config = env_vector_db_raw.copy()
    final_vector_db_config.update(file_vector_db)

    final_embedding_config = env_embedding_raw.copy()
    final_embedding_config.update(file_embedding)

    loaded_confluence_config: Optional[ConfluenceConfig] = None
    required_confluence_fields = ["url", "username", "api_token"]
    if all(field in final_confluence_config for field in required_confluence_fields):
        try:
            loaded_confluence_config = ConfluenceConfig(**final_confluence_config)
        except ValidationError as e:
            print(f"Error: Invalid Confluence configuration: {e}")
            print("Warning: Confluence client cannot be initialized.")
    else:
        print(
            "Info: Essential Confluence configuration (url, username, api_token) not found. Confluence client disabled."
        )

    try:
        loaded_search_config = SearchConfig(**final_search_config)
    except ValidationError as e:
        print(f"Error: Invalid Search configuration: {e}. Using defaults.")
        loaded_search_config = SearchConfig()

    loaded_embedding_config: Optional[EmbeddingConfig] = None
    if (
        final_embedding_config.get("provider", "none") != "none"
        or len(final_embedding_config) > 1
    ):
        if "provider" not in final_embedding_config:
            final_embedding_config["provider"] = "none"

        try:
            filtered_emb_config = {
                k: v for k, v in final_embedding_config.items() if v is not None
            }
            config_instance = EmbeddingConfig(**filtered_emb_config)
            if config_instance.provider != "none":
                loaded_embedding_config = config_instance
                print(
                    f"Info: Embedding Provider configured: Type='{config_instance.provider}', Model='{config_instance.model_name}', Dimension='{config_instance.dimension}'"
                )
            else:
                print("Info: Embedding provider is disabled (provider='none').")

        except (ValidationError, ValueError) as e:
            print(f"Error: Invalid Embedding configuration: {e}")
            print(
                "Warning: Embedding configuration failed. Embedding features may be disabled."
            )
    else:
        print("Info: No Embedding configuration found. Embedding features disabled.")

    if loaded_embedding_config and loaded_embedding_config.dimension is not None:
        if "embedding_dimension" not in final_vector_db_config:
            final_vector_db_config["embedding_dimension"] = (
                loaded_embedding_config.dimension
            )
            print(
                f"Info: Setting VectorDB embedding_dimension from EmbeddingConfig: {loaded_embedding_config.dimension}"
            )
        elif (
            final_vector_db_config.get("embedding_dimension")
            != loaded_embedding_config.dimension
        ):
            print(
                f"Warning: VECTOR_DB_EMBEDDING_DIMENSION ({final_vector_db_config.get('embedding_dimension')}) "
                f"differs from EMBEDDING_DIMENSION ({loaded_embedding_config.dimension}). Using the VectorDB specific value."
            )

    loaded_vector_db_config: Optional[VectorDBConfig] = None
    if (
        final_vector_db_config.get("type", "none") != "none"
        or len(final_vector_db_config) > 1
    ):
        if "type" not in final_vector_db_config:
            final_vector_db_config["type"] = "none"

        try:
            filtered_vdb_config = {
                k: v for k, v in final_vector_db_config.items() if v is not None
            }
            config_instance = VectorDBConfig(**filtered_vdb_config)
            if config_instance.type != "none":
                loaded_vector_db_config = config_instance
                print(
                    f"Info: Vector DB configured: Type='{config_instance.type}', Collection='{config_instance.collection_name}'"
                )
            else:
                print("Info: Vector database integration is disabled (type='none').")

        except (ValidationError, ValueError) as e:
            print(f"Error: Invalid Vector DB configuration: {e}")
            print(
                "Warning: Vector database configuration failed. Vector DB features will be disabled."
            )
    else:
        print("Info: No Vector DB configuration found. Vector DB features disabled.")

    return (
        loaded_confluence_config,
        loaded_search_config,
        loaded_vector_db_config,
        loaded_embedding_config,
    )


confluence_config, search_config, vector_db_config, embedding_config = (
    load_configurations()
)
